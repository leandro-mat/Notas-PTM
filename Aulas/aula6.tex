\chapter[Aula 6]{Variáveis Aleatórias e Independência}
\chaptermark{}

\section{Variáveis Aleatórias}

\begin{definicao}[Variável Aleatória]\label{def-var-aleatoria}
Seja $(\Omega,\F)$ um espaço de medida e $\Lambda\in\F$.
Uma função $X:\Lambda\to\overline{\R}$ tal que para todo 
$B\in\mathscr{B}(\overline{\R})$ temos 
	\[
		\{\omega \in\Lambda: X(\omega)\in B\} 
		\in \Lambda\cap\F,
	\]
onde $\Lambda\cap \F$ denota a coleção de todos os 
subconjuntos de $\Omega$ da forma $\Lambda\cap F$
com $F\in\F$.
\end{definicao}

\begin{observacao}
Esta definição nesta generalidade é necessária por razões
lógicas em algumas aplicações, mas para a discussão das
propriedades básicas de variáveis aleatórias, podemos 
supor que $\Lambda =\Omega$.
\end{observacao}


\begin{exercicio}
Suponha que $\Lambda=\Omega$ na Definição \ref{def-var-aleatoria}.
Mostre que uma variável aleatória é uma função $\F$-mensurável
tomando valores em $\overline{\R}$ no sentido da seção anterior.  
\end{exercicio}

Seja $(\Omega,\F,\P)$ um espaço de probabilidade. 
Se $X:\Omega\to\overline{\R}$ é uma variável aleatória então vamos 
usar a notação
	\[
		\P(X\in B) \equiv \P(\{\omega\in\Omega: X(\omega)\in B\}).
	\]
Vamos usar a abreviação v.a. para nos referir a uma variável aleatória
e ao invés de escrever $X:\Omega\to \overline{\R}$ é uma 
v.a., vamos escrever simplesmente $X$ é uma v.a..
Quando $X(\Omega)\subset \R$ vamos dizer que $X$ é uma 
v.a. real.





\begin{proposicao}
	Se $X$ uma v.a. real em $(\Omega,\F,\P)$ então 
	$\mu:\F\to [0,1]$ dada por 
		\[
			\mu(B)\equiv \P(X^{-1}(B))=\P(X\in B)
		\]
	é uma medida de probabilidade em 
	$(\R,\mathscr{B}(\R))$.
\end{proposicao}

\begin{proof}
Claramente $\mu(B)\geq 0$ para todo $B\in \mathscr{B}(\R)$. 
Se $\{A_n\}$ é uma sequência de conjuntos 
mutuamente disjunta em $\mathscr{B}(\R)$ então 
$\{X^{-1}(A_n)\}$ é uma sequência mutuamente disjunta em $\Omega$, 
portanto 
\begin{align*}
	\mu \left( \bigcup_{n=1}^{\infty} A_n   \right)
	=
	\P \left( X^{-1}\left(\bigcup_{n=1}^{\infty} A_n \right) \right)
	=
	\P \left( \bigcup_{n=1}^{\infty} X^{-1}(A_n) \right)
	=&
	\sum_{n=1}^{\infty}\P(X^{-1}(A_n))
	\\
	=&
	\sum_{n=1}^{\infty}\mu(A_n)
\end{align*}
Já que $X^{-1}(\R)=\Omega$, temos que $\mu(\R)=1$ e
isto encerra a prova de que $\mu$ é uma medida de probabilidade.
\end{proof}

A medida de probabilidade $\mu$ induzida pela v.a. real $X$, 
definida na proposição acima, é frequentemente denotada por 
	\[
		\mu = \P\circ X^{-1}.
	\]
Neste caso, a função distribuição $F$ associada 
a medida $\mu$ é chamada 
{\bf função distribuição de $X$}, 
\index{Função!Distribuição de $X$}
mais especificamente 
	\[
		F(x) = \mu((-\infty,x]) = \P(X\leq x).
	\] 
Quando estivermos lidando com mais de uma v.a. usamos 
a notação $F_X$ para indicar que estamos falando da 
função distribuição de $X$.


\begin{teorema}
	Seja $(\Omega,\F)$ um espaço mensurável. 
	Se $X$ é uma v.a. real e $f:\R\to\R$ é uma 
	função mensurável (com respeito a $\sigma$-álgebra de Borel), 
	então $f(X)$ é uma v.a. real.
\end{teorema}

\begin{proof}
 Segue das propriedades elementares de composição de função que 
 $(f(X))^{-1}(A) = (f\circ X)^{-1}(A) = X^{-1}(f^{-1}(A))$.
 Logo 
 \[
 	(f\circ X)^{-1}(\mathscr{B}(\R))
 	=
 	X^{-1}(f^{-1}(\mathscr{B}(\R)))
 	=
 	X^{-1}(\mathscr{B}(\R))
 	\subset 
 	\F.
 \]
O que completa a demostração.
\end{proof}









\section{Independência}

Independência é uma propriedade básica de eventos e variáveis 
aleatórias em vários modelos de probabilidade. A definição deste
conceito é motivada pelo raciocínio intuitivo de que a ocorrência 
ou não de um evento não afeta nossa estimativa da probabilidade 
que um evento independente ocorra ou não. 
Apesar deste conceito ter um  apelo 
intuitivo é importante entender que independência em Teoria 
da Probabilidade é um conceito técnico com uma definição 
precisa e que deve ser verificada em cada modelo específico
que estiver sendo estudado.
 
Certamente existem exemplos de eventos dependentes que nossa 
intuição nos diz que eles devem ser dependentes e exemplos que 
nossa intuição diz que não devem ser independentes, mas que 
satisfazem a definição. Assim devemos recorrer sempre a definição 
para termos certeza sobre a independência de determinados eventos.  


\begin{definicao}
\label{def-2-eventos-independentes}
\index{Eventos!Independentes}
%
	Seja $(\Omega,\F,\P)$ um espaço de probabilidade fixado. 
	Os eventos $A$ e $B$ são ditos independentes  se 
		\[
			\P(A\cap B) = \P(A) \P(B).
		\]
%
\end{definicao}
%
%
%
%
%
\begin{definicao}{def-n-eventos-independentes}
Seja $(\Omega,\F,\P)$ um espaço de probabilidade fixado.
Dizemos que os eventos $A_1,\ldots,A_n$ são independentes
se 
	\[
		\P\left( \bigcap_{i\in I} A_i \right)
		=
		\prod_{i\in I} \P(A_i),
		\qquad
		\forall I\subset \{1,\ldots,n\}.
	\]	
\end{definicao}

Deve se observar que a condição de independência 
de eventos $A_1,\ldots,A_n$ envolve a verificação de 
	\[
		\sum_{k=2}^n \binom{n}{k} = 2^n-n-1
	\]
equações. 

Definimos em seguida, o conceito de independência de uma 
quantidade finita de coleções de subconjuntos de um espaço
de probabilidade. Em grande parte das aplicações vamos usar
este conceito com cada coleção sendo uma $sigma$-álgebra 
de conjuntos de $\Omega$. 


\begin{definicao}
\label{def-n-colecoes-independentes}
Sejam $(\Omega,\F,\P)$ um espaço de probabilidade fixado
e $\mathcal{C}_i\subset \F$,  $i=1,\ldots,n$ 
coleções de eventos. 
Dizemos que as coleções $\mathcal{C}_i$'s são independentes
se para qualquer escolha de $A_1,\ldots,A_n$, 
com $A_i\in \mathcal{C}_i$, $i=1,\ldots,n$, temos que 
os eventos $A_1,\ldots,A_n$ são eventos independentes.
\end{definicao}

Prosseguimos apresentando um critério bastante útil
para provar independência de uma 
quantidade finita de sub-$\sigma$-álgebras de $\F$.
Devido a sua importância nesta seção, vamos apresentá-lo
na forma de um teorema.

\begin{teorema}
Seja $(\Omega,\F,\P)$ um espaço de probabilidade fixado.
Se $\mathcal{C}_i$ para $i=1,\ldots,n$ é uma 
coleção (não vazia) de {\bf eventos} tais que 
	\begin{enumerate}
		\item $\mathcal{C}_i	$ é um $\pi$-sistema;
		\item $\mathcal{C}_i,\ i=1,\ldots, n$ são independentes,
	\end{enumerate}	 	
então $\sigma(\mathcal{C}_1),\ldots, \sigma(\mathcal{C}_n)$ são 
independentes.
\end{teorema}




\begin{proof}
A prova deste teorema será feita por indução 
no número de coleções. 
Primeiro mostramos que a tese do teorema se 
verifica para o caso $n=2$. 
Fixe $A_2\in\mathcal{C}_2$ e defina
	\[
		\mathcal{L}
		=
		\{ A\in \F: \P(A\cap A_2) = \P(A)\P(A_2)\}.
	\]
Afirmamos que $\mathcal{L}$ é um $\lambda$-sistema.
De fato, primeiramente temos que $\Omega\in \mathcal{L}$ pois, 
$\P(\Omega\cap A_2)=\P(A_2)=\P(\Omega)\P(A_2)$.
A coleção $\mathcal{L}$ é fechada para complementação pois, 
para qualquer $A\in\mathcal{L}$, temos que
	\begin{align*}
	\P(A^c\cap A_2) 
	&=
	\P((\Omega\setminus A)\cap A_2)
	\\
	&=
	\P( A_2\setminus (A\cap A_2))
	\\
	&=
	\P( A_2) -\P(A\cap A_2)
	\\
	&=
	\P( A_2) -\P(A)\P(A_2)
	\\
	&=
	\P( A_2) (1-\P(A))
	\\
	&=
	\P(A_2)\P(A^c).
	\end{align*}
O que mostra que $A^c\in\mathcal{L}$. 
Para encerrar a prova da afimarção resta
mostrar que se $\{B_n\}$ uma coleção dois a dois disjunta 
em $\mathcal{L}$ então $\cup_{i=1}^{\infty} B_n\in\mathcal{L}$.
Este fato segue das seguintes igualdades
	\begin{align*}
	\P \left( A_2 \cap \bigcup_{n=1}^{\infty} B_n \right) 
	&=
	\P \left( \bigcup_{n=1}^{\infty} (A_2 \cap B_n) \right) 
	\\
	&=
	\sum_{n=1}^{\infty}\P \left( A_2 \cap B_n \right) 
	\\
	&=
	\sum_{n=1}^{\infty}\P(A_2)\P(B_n)
	\\
	&=
	\P(A_2)\sum_{n=1}^{\infty}\P(B_n)
	\\
	&=
	\P(A_2) \P \left( \bigcup_{n=1}^{\infty} B_n \right). 
	\end{align*}
%
%
%
Por hipótese $\mathcal{C}_1\subset \mathcal{L}$ 
e é um $\pi$-sistema.
Como mostramos que $\mathcal{L}$ é um 
$\lambda$-sistema podemos 
aplicar o Teorema de Dynkin para concluir que 
$\sigma(\mathcal{C}_1)\subset \mathcal{L}$.
Como $A_2$ é arbitrário em $\mathcal{C}_2$ 
temos mostrado neste momento que as coleções 
$\sigma(\mathcal{C}_1)$ e $\mathcal{C}_2$ 
são independentes.

Agora fixamos $A_1\in \sigma(\mathcal{C}_1)$ e
de maneira análoga definimos 
	\[
	\mathcal{K}=
	\{ A\in \F: \P(A\cap A_1) = \P(A)\P(A_1)\}
	\]
e verificamos que está coleção é um $\lambda$-sistema.
Como vimos acima, $\sigma(\mathcal{C}_1)$ e $\mathcal{C}_2$ 
são independentes logo $\mathcal{C}_2 \subset \mathcal{K}$.
Usando novamente o Teorema de Dynkin completamos a prova
do caso $n=2$. 

Claramente o passo de indução pode 
ser feito utilizando a técnica apresentada acima e 
assim o teorema está provado.
\end{proof}






\begin{definicao}[Coleções Independentes]
Sejam $(\Omega,\F,\P)$ um espaço de probabilidade e 
$I$ um conjunto de índices arbitrário.  
As coleções (de eventos) $\{\mathcal{C}_i\}_{i\in I}$
são independentes se para todo $J\subset I$ finito,
temos que as coleções $\{\mathcal{C}\}_{j\in J}$
são independentes.
\end{definicao}




\begin{center}
	{\red Em andamento.}
\end{center}


